{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the folder: 673\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = r'D:\\CSE463 Homework 04\\training images\\melanoma'\n",
    "\n",
    "image_count = sum([1 for file in os.listdir(folder_path) if file.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "print(f\"Number of images in the folder: {image_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 539 images belonging to 1 classes.\n",
      "Found 134 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dir = r\"D:\\CSE463 Homework 04\\training images\"\n",
    "img_size = 224  \n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255, \n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "train_data = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "def build_model(base_model):\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x) \n",
    "    x = Dense(256, activation='relu')(x) \n",
    "    x = Dense(1, activation='sigmoid')(x) \n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def train_model(model_name, base_model_fn, img_size):\n",
    "    print(f\"Training {model_name}...\")\n",
    "    base_model = base_model_fn(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n",
    "    model = build_model(base_model)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    " \n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        validation_data=val_data,\n",
    "        epochs=5, \n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    results = model.evaluate(val_data, verbose=1)\n",
    "    print(f\"{model_name} Validation Loss: {results[0]}, Validation Accuracy: {results[1]}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VGG16...\n",
      "Epoch 1/5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - accuracy: 1.0000 - loss: 0.0923"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 10s/step - accuracy: 1.0000 - loss: 0.0888 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 10s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 9s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 9s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 9s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "VGG16 Validation Loss: 0.0, Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Train VGG16\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "vgg16_model = train_model(\"VGG16\", VGG16, 224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNet50...\n",
      "Epoch 1/5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 6s/step - accuracy: 0.9126 - loss: 0.2118 - val_accuracy: 1.0000 - val_loss: 9.7968e-04\n",
      "Epoch 2/5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 6s/step - accuracy: 1.0000 - loss: 1.2929e-04 - val_accuracy: 1.0000 - val_loss: 4.1825e-05\n",
      "Epoch 3/5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 6s/step - accuracy: 1.0000 - loss: 4.1742e-05 - val_accuracy: 1.0000 - val_loss: 6.3108e-05\n",
      "Epoch 4/5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 6s/step - accuracy: 1.0000 - loss: 2.9396e-05 - val_accuracy: 1.0000 - val_loss: 7.0698e-05\n",
      "Epoch 5/5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 6s/step - accuracy: 1.0000 - loss: 3.5429e-05 - val_accuracy: 1.0000 - val_loss: 3.8392e-05\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.9428e-05\n",
      "ResNet50 Validation Loss: 3.885593105223961e-05, Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Train ResNet50\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "resnet50_model = train_model(\"ResNet50\", ResNet50, 224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 539 images belonging to 1 classes.\n",
      "Found 134 images belonging to 1 classes.\n",
      "Training MobileNetV2...\n",
      "Epoch 1/5\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 622ms/step - accuracy: 0.8023 - loss: 0.2874 - val_accuracy: 1.0000 - val_loss: 9.2304e-08\n",
      "Epoch 2/5\n",
      "\u001b[1m 1/16\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 402ms/step - accuracy: 1.0000 - loss: 1.2168e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 1.2168e-08 - val_accuracy: 1.0000 - val_loss: 7.1618e-08\n",
      "Epoch 3/5\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 532ms/step - accuracy: 1.0000 - loss: 1.9375e-09 - val_accuracy: 1.0000 - val_loss: 9.6163e-09\n",
      "Epoch 4/5\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 3.8992e-10 - val_accuracy: 1.0000 - val_loss: 9.2343e-09\n",
      "Epoch 5/5\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 531ms/step - accuracy: 1.0000 - loss: 2.8436e-10 - val_accuracy: 1.0000 - val_loss: 6.1743e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as melanoma_mobilenetv2.h5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 339ms/step - accuracy: 1.0000 - loss: 4.3046e-09\n",
      "Validation Loss: 5.912384803963278e-09\n",
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Paths\n",
    "train_dir = \"D:/CSE463 Homework 04/training images\"\n",
    "\n",
    "# Parameters\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,        # Rescale pixel values\n",
    "    validation_split=0.2      # 20% validation split\n",
    ")\n",
    "\n",
    "# Load Training Data\n",
    "train_data = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',     \n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load Validation Data\n",
    "val_data = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',      \n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Load Pretrained MobileNetV2\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  \n",
    "\n",
    "# Add Custom Classification Head\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  # Global Average Pooling\n",
    "x = Dense(128, activation='relu')(x)  # Fully connected layer\n",
    "output = Dense(1, activation='sigmoid')(x)  # Output layer for binary classification\n",
    "\n",
    "# Create the Model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Training MobileNetV2...\")\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=train_data.samples // batch_size,\n",
    "    validation_steps=val_data.samples // batch_size\n",
    ")\n",
    "\n",
    "model.save(\"melanoma_mobilenetv2.h5\")\n",
    "print(\"Model saved as melanoma_mobilenetv2.h5\")\n",
    "\n",
    "loss, accuracy = model.evaluate(val_data)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = tf.keras.models.load_model('path_to_vgg16_model')  # TensorFlow SavedModel format\n",
    "resnet50_model = tf.keras.models.load_model('path_to_resnet50_model')  # TensorFlow SavedModel format\n",
    "mobilenetv2_model = tf.keras.models.load_model('path_to_mobilenetv2_model')  # TensorFlow SavedModel format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define model filenames\n",
    "model_files = [\"vgg16_melanoma.h5\", \"resnet50_melanoma.h5\", \"melanoma_mobilenetv2.h5\"]\n",
    "\n",
    "# Verify model files exist\n",
    "missing_files = [file for file in model_files if not os.path.exists(file)]\n",
    "if missing_files:\n",
    "    print(f\"Missing model files: {missing_files}\")\n",
    "    print(\"Please ensure the models are trained and saved correctly.\")\n",
    "    exit()\n",
    "\n",
    "# Load models\n",
    "vgg16_model = tf.keras.models.load_model(\"vgg16_melanoma.h5\")\n",
    "resnet50_model = tf.keras.models.load_model(\"resnet50_melanoma.h5\")\n",
    "mobilenetv2_model = tf.keras.models.load_model(\"melanoma_mobilenetv2.h5\")\n",
    "\n",
    "# Load dataset for evaluation\n",
    "data_dir = \"D:/CSE463 Homework 04/training images\"  # Update with your dataset path\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    image_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Extract images and labels for metrics evaluation\n",
    "images = []\n",
    "labels = []\n",
    "for batch_images, batch_labels in dataset:\n",
    "    images.append(batch_images.numpy())\n",
    "    labels.append(batch_labels.numpy())\n",
    "images = np.concatenate(images, axis=0)\n",
    "labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(model, model_name):\n",
    "    predictions = model.predict(images)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predicted_classes)\n",
    "    precision = precision_score(labels, predicted_classes, average=\"weighted\", zero_division=0)\n",
    "    recall = recall_score(labels, predicted_classes, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(labels, predicted_classes, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, predicted_classes, zero_division=0))\n",
    "\n",
    "# Evaluate each model\n",
    "evaluate_model(vgg16_model, \"VGG16\")\n",
    "evaluate_model(resnet50_model, \"ResNet50\")\n",
    "evaluate_model(mobilenetv2_model, \"MobileNetV2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "as we can see from the results, the MobileNetV2 model has the highest accuracy, precision, recall, and F1 score among the three models.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
